{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ramit/anaconda2/envs/kera36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ramit/anaconda2/envs/kera36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1044: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Loaded Model from disk\n",
      "WARNING:tensorflow:From /home/ramit/anaconda2/envs/kera36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#for matrix math\n",
    "import numpy as np\n",
    "#for importing our keras model\n",
    "import keras.models\n",
    "#for regular expressions, saves time dealing with string data\n",
    "import re\n",
    "import keras\n",
    "from keras.layers import LSTM,Embedding,Dropout,Activation,Dense,Bidirectional,GRU\n",
    "from keras.models import Sequential,Input,Model\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "import time\n",
    "\n",
    "#system level operations (like loading files)\n",
    "import sys \n",
    "#for reading operating system data\n",
    "import os\n",
    "#tell our app where our saved model is\n",
    "#sys.path.append(os.path.abspath(\"./model\"))\n",
    "from load import * \n",
    "#initalize our flask app\n",
    "#app = Flask(__name__)\n",
    "#global vars for easy reusability\n",
    "global model, graph\n",
    "#initialize these variables\n",
    "model, graph = init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract glove.6B.50d.txt\n",
      "Embeddings size: 400000\n",
      "Found 9842 samples.\n",
      "Found 6368 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "glove_filename = 'glove.6B.50d.txt'\n",
    "\n",
    "\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "print('Extract %s' % glove_filename)\n",
    "with open(glove_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:],dtype=np.float32)\n",
    "        embeddings[word] = embedding\n",
    "        \n",
    "print('Embeddings size: %d' % len(embeddings))\n",
    "#print(embeddings)\n",
    "import json\n",
    "def loadDataset(filename, size=100):\n",
    "    label_category = {\n",
    "        'neutral': 0,\n",
    "        'entailment': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    dataset = []\n",
    "    sentence1 = []\n",
    "    sentence2 = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        not_found = 0\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            label = row['gold_label'].strip()\n",
    "            if label in label_category:\n",
    "                labels.append(label_category[label])\n",
    "                sentence1.append(row['sentence1'].strip())\n",
    "                sentence2.append(row['sentence2'].strip())\n",
    "    return to_categorical(labels),sentence1,sentence2\n",
    "                   \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "labels,train_sentence1,train_sentence2 =  loadDataset('snli_1.0/snli_1.0_dev.jsonl', 100) \n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "labels\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "NUM_WORDS = len(embeddings) #200000\n",
    "\n",
    "print('Found %s samples.' % len(train_sentence1))\n",
    "\n",
    "train_sentences = train_sentence1 + train_sentence2\n",
    "\n",
    "tokenizer = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "sentence1_word_sequences = tokenizer.texts_to_sequences(train_sentence1)\n",
    "sentence2_word_sequences = tokenizer.texts_to_sequences(train_sentence2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(a):\n",
    "    df = pd.read_csv(a)\n",
    "    \n",
    "    texts = df['sentence1']\n",
    "    hypotheses = df['sentence2 ']\n",
    "    #label = df[2]\n",
    "    text = []\n",
    "    hypothesis = []\n",
    "\n",
    "    \n",
    "    for i in texts:\n",
    "        text.append(i)\n",
    "    for i in hypotheses:\n",
    "        hypothesis.append(i)\n",
    "    return text, hypothesis\n",
    "max_seq_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry ate six shrimp at dinner.', 'Beautiful giraffes roam the savannah.', 'Sue changed the flat tire.', 'We are going to watch a movie tonight.', 'I ran the obstacle course in record time.', 'The crew paved the entire stretch of highway.', 'Mom read the novel in one day.', 'I will clean the house every Saturday.', 'The staff is required to watch a safety video every year.', 'Tom painted the entire house.', \"The teacher always answers the students' questions.\", 'The choir really enjoys that piece.', 'The forest fire destroyed the whole suburb.', 'The two kings are signing the treaty.', 'The cleaning crew vacuums and dusts the office every night.', 'Larry generously donated money to the homeless shelter.', 'The wedding planner is making all the reservations.', 'Susan will bake two dozen cupcakes for the bake sale.', 'The science class viewed the comet.', 'The director will give you instructions.', 'Thousands of tourists visit the Grand Canyon every year.', 'The homeowners remodeled the house to help it sell.', 'The saltwater corroded the metal beams.', 'The kangaroo carried her baby in her pouch.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_sentence1,train_sentence2 = load_data('activepassive.csv')\n",
    "print(train_sentence1)\n",
    "sentence1_word_sequences = tokenizer.texts_to_sequences(train_sentence1)\n",
    "sentence2_word_sequences = tokenizer.texts_to_sequences(train_sentence2)\n",
    "s1_data = pad_sequences(sentence1_word_sequences, maxlen = max_seq_length)\n",
    "s2_data = pad_sequences(sentence2_word_sequences, maxlen = max_seq_length)\n",
    "y = model.predict([s1_data, s2_data],verbose = 0)\n",
    "n_values = 3\n",
    "t = []\n",
    "c = np.eye(n_values, dtype=int)[np.argmax(y, axis=1)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "for i in c:\n",
    "    if str(i) == \"[1 0 0]\":\n",
    "        print('neutral')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,url_for,request\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "#for importing our keras model\n",
    "import keras.models\n",
    "#for regular expressions, saves time dealing with string data\n",
    "import re\n",
    "import keras\n",
    "from keras.layers import LSTM,Embedding,Dropout,Activation,Dense,Bidirectional,GRU\n",
    "from keras.models import Sequential,Input,Model\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "import time\n",
    "\n",
    "#system level operations (like loading files)\n",
    "import sys \n",
    "#for reading operating system data\n",
    "import os\n",
    "#tell our app where our saved model is\n",
    "#sys.path.append(os.path.abspath(\"./model\"))\n",
    "from load import * \n",
    "#initalize our flask app\n",
    "#app = Flask(__name__)\n",
    "#global vars for easy reusability\n",
    "global model, graph\n",
    "#initialize these variables\n",
    "model, graph = init()\n",
    "import numpy as np\n",
    "glove_filename = 'glove.6B.50d.txt'\n",
    "\n",
    "\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "print('Extract %s' % glove_filename)\n",
    "with open(glove_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:],dtype=np.float32)\n",
    "        embeddings[word] = embedding\n",
    "        \n",
    "print('Embeddings size: %d' % len(embeddings))\n",
    "#print(embeddings)\n",
    "import json\n",
    "def loadDataset(filename, size=100):\n",
    "    label_category = {\n",
    "        'neutral': 0,\n",
    "        'entailment': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    dataset = []\n",
    "    sentence1 = []\n",
    "    sentence2 = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        not_found = 0\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            label = row['gold_label'].strip()\n",
    "            if label in label_category:\n",
    "                labels.append(label_category[label])\n",
    "                sentence1.append(row['sentence1'].strip())\n",
    "                sentence2.append(row['sentence2'].strip())\n",
    "    return to_categorical(labels),sentence1,sentence2\n",
    "                   \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "labels,train_sentence1,train_sentence2 =  loadDataset('snli_1.0/snli_1.0_dev.jsonl', 100) \n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "labels\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "NUM_WORDS = len(embeddings) #200000\n",
    "\n",
    "print('Found %s samples.' % len(train_sentence1))\n",
    "\n",
    "train_sentences = train_sentence1 + train_sentence2\n",
    "\n",
    "tokenizer = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "sentence1_word_sequences = tokenizer.texts_to_sequences(train_sentence1)\n",
    "sentence2_word_sequences = tokenizer.texts_to_sequences(train_sentence2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "def load_data(a):\n",
    "    df = pd.read_csv(a)\n",
    "    \n",
    "    texts = df['sentence1']\n",
    "    hypotheses = df['sentence2 ']\n",
    "    #label = df[2]\n",
    "    text = []\n",
    "    hypothesis = []\n",
    "\n",
    "    \n",
    "    for i in texts:\n",
    "        text.append(i)\n",
    "    for i in hypotheses:\n",
    "        hypothesis.append(i)\n",
    "    return text, hypothesis\n",
    "max_seq_length = 20\n",
    "app = Flask(__name__)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "\treturn render_template('home.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "\n",
    "\tif request.method == 'POST':\n",
    "\t\ttext = request.form['message']\n",
    "        train_sentence1 = list(text)\n",
    "    if request.method == 'SPOST':\n",
    "        hypothesis = request.form['messages']\n",
    "        train_sentence2 = list(hypothesis)\n",
    "    sentence1_word_sequences = tokenizer.texts_to_sequences(train_sentence1)\n",
    "    sentence2_word_sequences = tokenizer.texts_to_sequences(train_sentence2)\n",
    "    s1_data = pad_sequences(sentence1_word_sequences, maxlen = max_seq_length)\n",
    "    s2_data = pad_sequences(sentence2_word_sequences, maxlen = max_seq_length)\n",
    "    y = model.predict([s1_data, s2_data],verbose = 0)\n",
    "    n_values = 3\n",
    "    t = []\n",
    "    c = np.eye(n_values, dtype=int)[np.argmax(y, axis=1)]\n",
    "    if str(c) == \"[1 0 0]\":\n",
    "        k = 'entailment'\n",
    "    if str(c) == \"[0 1 0]\":\n",
    "        k = 'contradiction'\n",
    "    if str(c) == \"[0 0 1]\":\n",
    "        k = 'neutral'\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\treturn render_template('result.html',prediction = k)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tapp.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
